2025-05-25 12:07:32,603 - INFO - ==================================================
2025-05-25 12:07:32,605 - INFO - Training Parameters:
2025-05-25 12:07:32,605 - INFO - batch_size: 8
2025-05-25 12:07:32,605 - INFO - dist_url: env://
2025-05-25 12:07:32,605 - INFO - epochs: 50
2025-05-25 12:07:32,606 - INFO - exp_name: 0524_panorama_images1_gpu4_ep50_bs32_lr2e-5
2025-05-25 12:07:32,606 - INFO - freeze_backbone: False
2025-05-25 12:07:32,606 - INFO - image_path: /home/zhongxinyu/capstone/HK/datasets/panorama_images_1
2025-05-25 12:07:32,606 - INFO - label_path: /home/zhongxinyu/capstone/HK/datasets/sampled_labels.csv
2025-05-25 12:07:32,606 - INFO - log_dir: /home/zhongxinyu/capstone/HK/DeepLabV3Plus-Pytorch-master/logs
2025-05-25 12:07:32,606 - INFO - log_interval: 10
2025-05-25 12:07:32,606 - INFO - lr: 0.0001
2025-05-25 12:07:32,606 - INFO - master_addr: localhost
2025-05-25 12:07:32,607 - INFO - master_port: 12345
2025-05-25 12:07:32,607 - INFO - min_lr: 1e-07
2025-05-25 12:07:32,607 - INFO - nproc_per_node: 4
2025-05-25 12:07:32,607 - INFO - pretrained: /home/zhongxinyu/capstone/HK/DeepLabV3Plus-Pytorch-master/checkpoints/best_deeplabv3plus_resnet101_cityscapes_os16.pth.tar
2025-05-25 12:07:32,607 - INFO - rank: 0
2025-05-25 12:07:32,607 - INFO - save_path: /home/zhongxinyu/capstone/HK/DeepLabV3Plus-Pytorch-master/checkpoints/best_regression_model.pth
2025-05-25 12:07:32,607 - INFO - warmup_epochs: 1
2025-05-25 12:07:32,607 - INFO - world_size: 4
2025-05-25 12:07:32,608 - INFO - Date: 2025-05-25 12:07:32
2025-05-25 12:07:32,608 - INFO - PyTorch Version: 1.12.0
2025-05-25 12:07:32,608 - INFO - GPU Count: 8
2025-05-25 12:07:32,615 - INFO - GPU 0: Tesla V100-PCIE-32GB
2025-05-25 12:07:32,615 - INFO - GPU 1: Tesla V100-PCIE-32GB
2025-05-25 12:07:32,616 - INFO - GPU 2: Tesla V100-PCIE-32GB
2025-05-25 12:07:32,616 - INFO - GPU 3: Tesla V100-PCIE-32GB
2025-05-25 12:07:32,616 - INFO - GPU 4: Tesla V100-PCIE-32GB
2025-05-25 12:07:32,616 - INFO - GPU 5: Tesla V100-PCIE-32GB
2025-05-25 12:07:32,616 - INFO - GPU 6: Tesla V100-PCIE-32GB
2025-05-25 12:07:32,616 - INFO - GPU 7: Tesla V100-PCIE-32GB
2025-05-25 12:07:32,616 - INFO - ==================================================
2025-05-25 12:07:41,263 - INFO - 预训练权重加载完成: /home/zhongxinyu/capstone/HK/DeepLabV3Plus-Pytorch-master/checkpoints/best_deeplabv3plus_resnet101_cityscapes_os16.pth.tar
2025-05-25 12:07:48,887 - INFO - Epoch 1/50 | Learning Rate: 0.00e+00
2025-05-25 12:07:54,329 - INFO - Step 10 | Batch 10/176 | Train Loss: 1.4884 | LR: 0.00e+00
2025-05-25 12:07:59,385 - INFO - Step 20 | Batch 20/176 | Train Loss: 0.6084 | LR: 0.00e+00
2025-05-25 12:08:04,300 - INFO - Step 30 | Batch 30/176 | Train Loss: 1.9566 | LR: 0.00e+00
2025-05-25 12:08:09,270 - INFO - Step 40 | Batch 40/176 | Train Loss: 1.0547 | LR: 0.00e+00
2025-05-25 12:08:14,347 - INFO - Step 50 | Batch 50/176 | Train Loss: 1.0923 | LR: 0.00e+00
2025-05-25 12:08:19,622 - INFO - Step 60 | Batch 60/176 | Train Loss: 0.6477 | LR: 0.00e+00
2025-05-25 12:08:25,658 - INFO - Step 70 | Batch 70/176 | Train Loss: 0.0109 | LR: 0.00e+00
2025-05-25 12:08:30,765 - INFO - Step 80 | Batch 80/176 | Train Loss: 0.8808 | LR: 0.00e+00
2025-05-25 12:08:35,815 - INFO - Step 90 | Batch 90/176 | Train Loss: 0.1457 | LR: 0.00e+00
2025-05-25 12:08:40,742 - INFO - Step 100 | Batch 100/176 | Train Loss: 1.7336 | LR: 0.00e+00
2025-05-25 12:08:45,459 - INFO - Step 110 | Batch 110/176 | Train Loss: 1.4377 | LR: 0.00e+00
2025-05-25 12:08:50,580 - INFO - Step 120 | Batch 120/176 | Train Loss: 1.7281 | LR: 0.00e+00
2025-05-25 12:08:55,620 - INFO - Step 130 | Batch 130/176 | Train Loss: 0.2057 | LR: 0.00e+00
2025-05-25 12:09:00,880 - INFO - Step 140 | Batch 140/176 | Train Loss: 0.4080 | LR: 0.00e+00
2025-05-25 12:09:05,880 - INFO - Step 150 | Batch 150/176 | Train Loss: 2.1026 | LR: 0.00e+00
2025-05-25 12:09:11,123 - INFO - Step 160 | Batch 160/176 | Train Loss: 0.9304 | LR: 0.00e+00
2025-05-25 12:09:16,480 - INFO - Step 170 | Batch 170/176 | Train Loss: 1.4646 | LR: 0.00e+00
2025-05-25 12:09:29,745 - INFO - Validation Loss: 0.9077
2025-05-25 12:09:30,508 - INFO - New best model saved with val_loss: 0.9903
2025-05-25 12:09:30,510 - INFO - Epoch 1/50 Summary:
2025-05-25 12:09:30,511 - INFO - Train Loss: 1.0623 | Val Loss: 0.9903 | Best Val Loss: 0.9903
2025-05-25 12:09:30,511 - INFO - --------------------------------------------------
2025-05-25 12:09:30,513 - INFO - Epoch 2/50 | Learning Rate: 4.00e-04
2025-05-25 12:09:35,517 - INFO - Step 186 | Batch 10/176 | Train Loss: 2.5125 | LR: 4.00e-04
2025-05-25 12:09:40,371 - INFO - Step 196 | Batch 20/176 | Train Loss: 0.3022 | LR: 4.00e-04
2025-05-25 12:09:45,359 - INFO - Step 206 | Batch 30/176 | Train Loss: 2.0907 | LR: 4.00e-04
2025-05-25 12:09:50,404 - INFO - Step 216 | Batch 40/176 | Train Loss: 0.8359 | LR: 4.00e-04
2025-05-25 12:09:55,630 - INFO - Step 226 | Batch 50/176 | Train Loss: 0.7233 | LR: 4.00e-04
2025-05-25 12:10:00,721 - INFO - Step 236 | Batch 60/176 | Train Loss: 1.3867 | LR: 4.00e-04
2025-05-25 12:10:05,918 - INFO - Step 246 | Batch 70/176 | Train Loss: 3.3440 | LR: 4.00e-04
2025-05-25 12:10:11,247 - INFO - Step 256 | Batch 80/176 | Train Loss: 0.2600 | LR: 4.00e-04
2025-05-25 12:10:16,725 - INFO - Step 266 | Batch 90/176 | Train Loss: 0.2388 | LR: 4.00e-04
2025-05-25 12:10:23,146 - INFO - Step 276 | Batch 100/176 | Train Loss: 0.0109 | LR: 4.00e-04
2025-05-25 12:10:28,570 - INFO - Step 286 | Batch 110/176 | Train Loss: 1.0510 | LR: 4.00e-04
2025-05-25 12:10:33,132 - INFO - Step 296 | Batch 120/176 | Train Loss: 0.7388 | LR: 4.00e-04
2025-05-25 12:10:37,811 - INFO - Step 306 | Batch 130/176 | Train Loss: 2.0526 | LR: 4.00e-04
2025-05-25 12:10:42,943 - INFO - Step 316 | Batch 140/176 | Train Loss: 2.6895 | LR: 4.00e-04
2025-05-25 12:10:47,573 - INFO - Step 326 | Batch 150/176 | Train Loss: 0.7620 | LR: 4.00e-04
2025-05-25 12:10:52,745 - INFO - Step 336 | Batch 160/176 | Train Loss: 0.2482 | LR: 4.00e-04
2025-05-25 12:10:57,906 - INFO - Step 346 | Batch 170/176 | Train Loss: 0.8756 | LR: 4.00e-04
2025-05-25 12:11:08,856 - INFO - Validation Loss: 75.6158
2025-05-25 12:11:08,960 - INFO - Epoch 2/50 Summary:
2025-05-25 12:11:08,960 - INFO - Train Loss: 1.1833 | Val Loss: 19.6541 | Best Val Loss: 0.9903
2025-05-25 12:11:08,960 - INFO - --------------------------------------------------
2025-05-25 12:11:08,963 - INFO - Epoch 3/50 | Learning Rate: 4.00e-04
2025-05-25 12:11:14,095 - INFO - Step 362 | Batch 10/176 | Train Loss: 0.1009 | LR: 4.00e-04
2025-05-25 12:11:19,296 - INFO - Step 372 | Batch 20/176 | Train Loss: 1.9752 | LR: 4.00e-04
2025-05-25 12:11:25,705 - INFO - Step 382 | Batch 30/176 | Train Loss: 1.0515 | LR: 4.00e-04
2025-05-25 12:11:30,838 - INFO - Step 392 | Batch 40/176 | Train Loss: 0.2782 | LR: 4.00e-04
2025-05-25 12:11:35,866 - INFO - Step 402 | Batch 50/176 | Train Loss: 1.3349 | LR: 4.00e-04
2025-05-25 12:11:40,871 - INFO - Step 412 | Batch 60/176 | Train Loss: 0.0481 | LR: 4.00e-04
2025-05-25 12:11:45,989 - INFO - Step 422 | Batch 70/176 | Train Loss: 0.2708 | LR: 4.00e-04
2025-05-25 12:11:51,227 - INFO - Step 432 | Batch 80/176 | Train Loss: 0.3768 | LR: 4.00e-04
2025-05-25 12:11:56,691 - INFO - Step 442 | Batch 90/176 | Train Loss: 0.6907 | LR: 4.00e-04
2025-05-25 12:12:01,998 - INFO - Step 452 | Batch 100/176 | Train Loss: 0.6650 | LR: 4.00e-04
2025-05-25 12:12:07,051 - INFO - Step 462 | Batch 110/176 | Train Loss: 2.7685 | LR: 4.00e-04
2025-05-25 12:12:11,842 - INFO - Step 472 | Batch 120/176 | Train Loss: 1.1772 | LR: 4.00e-04
2025-05-25 12:12:16,667 - INFO - Step 482 | Batch 130/176 | Train Loss: 0.2244 | LR: 4.00e-04
2025-05-25 12:12:23,114 - INFO - Step 492 | Batch 140/176 | Train Loss: 0.0573 | LR: 4.00e-04
2025-05-25 12:12:28,575 - INFO - Step 502 | Batch 150/176 | Train Loss: 1.1048 | LR: 4.00e-04
2025-05-25 12:12:33,682 - INFO - Step 512 | Batch 160/176 | Train Loss: 1.4855 | LR: 4.00e-04
2025-05-25 12:12:38,531 - INFO - Step 522 | Batch 170/176 | Train Loss: 0.4170 | LR: 4.00e-04
2025-05-25 12:12:50,046 - INFO - Validation Loss: 346.1572
2025-05-25 12:12:50,055 - INFO - Epoch 3/50 Summary:
2025-05-25 12:12:50,055 - INFO - Train Loss: 1.0536 | Val Loss: 87.2889 | Best Val Loss: 0.9903
2025-05-25 12:12:50,055 - INFO - --------------------------------------------------
2025-05-25 12:12:50,057 - INFO - Epoch 4/50 | Learning Rate: 3.98e-04
2025-05-25 12:12:54,793 - INFO - Step 538 | Batch 10/176 | Train Loss: 0.1116 | LR: 3.98e-04
2025-05-25 12:13:00,168 - INFO - Step 548 | Batch 20/176 | Train Loss: 0.3292 | LR: 3.98e-04
2025-05-25 12:13:04,902 - INFO - Step 558 | Batch 30/176 | Train Loss: 0.1588 | LR: 3.98e-04
2025-05-25 12:13:09,715 - INFO - Step 568 | Batch 40/176 | Train Loss: 1.6666 | LR: 3.98e-04
2025-05-25 12:13:14,877 - INFO - Step 578 | Batch 50/176 | Train Loss: 1.0967 | LR: 3.98e-04
2025-05-25 12:13:20,076 - INFO - Step 588 | Batch 60/176 | Train Loss: 0.1810 | LR: 3.98e-04
2025-05-25 12:13:26,186 - INFO - Step 598 | Batch 70/176 | Train Loss: 1.7583 | LR: 3.98e-04
2025-05-25 12:13:31,221 - INFO - Step 608 | Batch 80/176 | Train Loss: 0.5886 | LR: 3.98e-04
2025-05-25 12:13:36,188 - INFO - Step 618 | Batch 90/176 | Train Loss: 1.8381 | LR: 3.98e-04
2025-05-25 12:13:42,845 - INFO - Step 628 | Batch 100/176 | Train Loss: 1.1571 | LR: 3.98e-04
2025-05-25 12:13:49,441 - INFO - Step 638 | Batch 110/176 | Train Loss: 0.0492 | LR: 3.98e-04
2025-05-25 12:13:55,578 - INFO - Step 648 | Batch 120/176 | Train Loss: 0.9188 | LR: 3.98e-04
2025-05-25 12:14:00,516 - INFO - Step 658 | Batch 130/176 | Train Loss: 1.9659 | LR: 3.98e-04
2025-05-25 12:14:05,669 - INFO - Step 668 | Batch 140/176 | Train Loss: 0.0635 | LR: 3.98e-04
2025-05-25 12:14:10,832 - INFO - Step 678 | Batch 150/176 | Train Loss: 0.4648 | LR: 3.98e-04
2025-05-25 12:14:16,032 - INFO - Step 688 | Batch 160/176 | Train Loss: 3.4770 | LR: 3.98e-04
2025-05-25 12:14:21,870 - INFO - Step 698 | Batch 170/176 | Train Loss: 0.4614 | LR: 3.98e-04
2025-05-25 12:14:32,548 - INFO - Validation Loss: 0.9796
2025-05-25 12:14:32,556 - INFO - Epoch 4/50 Summary:
2025-05-25 12:14:32,556 - INFO - Train Loss: 1.0845 | Val Loss: 0.9932 | Best Val Loss: 0.9903
2025-05-25 12:14:32,557 - INFO - --------------------------------------------------
2025-05-25 12:14:32,559 - INFO - Epoch 5/50 | Learning Rate: 3.96e-04
2025-05-25 12:14:37,251 - INFO - Step 714 | Batch 10/176 | Train Loss: 0.5426 | LR: 3.96e-04
2025-05-25 12:14:42,715 - INFO - Step 724 | Batch 20/176 | Train Loss: 2.1901 | LR: 3.96e-04
2025-05-25 12:14:48,636 - INFO - Step 734 | Batch 30/176 | Train Loss: 0.0171 | LR: 3.96e-04
2025-05-25 12:14:54,189 - INFO - Step 744 | Batch 40/176 | Train Loss: 0.3613 | LR: 3.96e-04
2025-05-25 12:14:59,790 - INFO - Step 754 | Batch 50/176 | Train Loss: 3.3991 | LR: 3.96e-04
2025-05-25 12:15:05,541 - INFO - Step 764 | Batch 60/176 | Train Loss: 1.2949 | LR: 3.96e-04
2025-05-25 12:15:12,206 - INFO - Step 774 | Batch 70/176 | Train Loss: 0.3671 | LR: 3.96e-04
2025-05-25 12:15:19,927 - INFO - Step 784 | Batch 80/176 | Train Loss: 2.1236 | LR: 3.96e-04
2025-05-25 12:15:27,889 - INFO - Step 794 | Batch 90/176 | Train Loss: 0.4003 | LR: 3.96e-04
2025-05-25 12:15:35,051 - INFO - Step 804 | Batch 100/176 | Train Loss: 1.1313 | LR: 3.96e-04
2025-05-25 12:15:42,246 - INFO - Step 814 | Batch 110/176 | Train Loss: 3.3695 | LR: 3.96e-04
2025-05-25 12:15:49,084 - INFO - Step 824 | Batch 120/176 | Train Loss: 0.5117 | LR: 3.96e-04
2025-05-25 12:15:55,825 - INFO - Step 834 | Batch 130/176 | Train Loss: 1.5698 | LR: 3.96e-04
2025-05-25 12:16:01,779 - INFO - Step 844 | Batch 140/176 | Train Loss: 0.8067 | LR: 3.96e-04
2025-05-25 12:16:07,813 - INFO - Step 854 | Batch 150/176 | Train Loss: 0.5307 | LR: 3.96e-04
2025-05-25 12:16:14,016 - INFO - Step 864 | Batch 160/176 | Train Loss: 0.8581 | LR: 3.96e-04
2025-05-25 12:16:20,017 - INFO - Step 874 | Batch 170/176 | Train Loss: 2.5110 | LR: 3.96e-04
2025-05-25 12:16:32,963 - INFO - Validation Loss: 1.0031
2025-05-25 12:16:32,970 - INFO - Epoch 5/50 Summary:
2025-05-25 12:16:32,970 - INFO - Train Loss: 1.1639 | Val Loss: 0.9989 | Best Val Loss: 0.9903
2025-05-25 12:16:32,970 - INFO - --------------------------------------------------
2025-05-25 12:16:32,973 - INFO - Epoch 6/50 | Learning Rate: 3.93e-04
2025-05-25 12:16:38,017 - INFO - Step 890 | Batch 10/176 | Train Loss: 0.9509 | LR: 3.93e-04
2025-05-25 12:16:43,119 - INFO - Step 900 | Batch 20/176 | Train Loss: 1.2329 | LR: 3.93e-04
2025-05-25 12:16:48,413 - INFO - Step 910 | Batch 30/176 | Train Loss: 0.0203 | LR: 3.93e-04
2025-05-25 12:16:53,574 - INFO - Step 920 | Batch 40/176 | Train Loss: 0.0753 | LR: 3.93e-04
2025-05-25 12:16:58,617 - INFO - Step 930 | Batch 50/176 | Train Loss: 0.9496 | LR: 3.93e-04
2025-05-25 12:17:03,582 - INFO - Step 940 | Batch 60/176 | Train Loss: 1.3072 | LR: 3.93e-04
2025-05-25 12:17:08,541 - INFO - Step 950 | Batch 70/176 | Train Loss: 1.8817 | LR: 3.93e-04
2025-05-25 12:17:13,634 - INFO - Step 960 | Batch 80/176 | Train Loss: 2.8130 | LR: 3.93e-04
2025-05-25 12:17:18,722 - INFO - Step 970 | Batch 90/176 | Train Loss: 0.6208 | LR: 3.93e-04
2025-05-25 12:17:25,323 - INFO - Step 980 | Batch 100/176 | Train Loss: 2.5144 | LR: 3.93e-04
2025-05-25 12:17:30,442 - INFO - Step 990 | Batch 110/176 | Train Loss: 0.3830 | LR: 3.93e-04
2025-05-25 12:17:35,458 - INFO - Step 1000 | Batch 120/176 | Train Loss: 0.8309 | LR: 3.93e-04
2025-05-25 12:17:40,629 - INFO - Step 1010 | Batch 130/176 | Train Loss: 1.8310 | LR: 3.93e-04
2025-05-25 12:17:45,836 - INFO - Step 1020 | Batch 140/176 | Train Loss: 0.0717 | LR: 3.93e-04
2025-05-25 12:17:50,835 - INFO - Step 1030 | Batch 150/176 | Train Loss: 1.0345 | LR: 3.93e-04
2025-05-25 12:17:55,980 - INFO - Step 1040 | Batch 160/176 | Train Loss: 0.2443 | LR: 3.93e-04
2025-05-25 12:18:01,221 - INFO - Step 1050 | Batch 170/176 | Train Loss: 1.8057 | LR: 3.93e-04
2025-05-25 12:18:12,613 - INFO - Validation Loss: 1.0008
2025-05-25 12:18:12,705 - INFO - Epoch 6/50 Summary:
2025-05-25 12:18:12,705 - INFO - Train Loss: 0.9861 | Val Loss: 1.0056 | Best Val Loss: 0.9903
2025-05-25 12:18:12,705 - INFO - --------------------------------------------------
2025-05-25 12:18:12,708 - INFO - Epoch 7/50 | Learning Rate: 3.90e-04
2025-05-25 12:18:17,516 - INFO - Step 1066 | Batch 10/176 | Train Loss: 1.8650 | LR: 3.90e-04
2025-05-25 12:18:23,742 - INFO - Step 1076 | Batch 20/176 | Train Loss: 2.4351 | LR: 3.90e-04
2025-05-25 12:18:28,906 - INFO - Step 1086 | Batch 30/176 | Train Loss: 1.0323 | LR: 3.90e-04
2025-05-25 12:18:34,080 - INFO - Step 1096 | Batch 40/176 | Train Loss: 0.5498 | LR: 3.90e-04
2025-05-25 12:18:39,304 - INFO - Step 1106 | Batch 50/176 | Train Loss: 0.0016 | LR: 3.90e-04
2025-05-25 12:18:44,404 - INFO - Step 1116 | Batch 60/176 | Train Loss: 0.4996 | LR: 3.90e-04
2025-05-25 12:18:49,686 - INFO - Step 1126 | Batch 70/176 | Train Loss: 1.1934 | LR: 3.90e-04
2025-05-25 12:18:54,707 - INFO - Step 1136 | Batch 80/176 | Train Loss: 0.3481 | LR: 3.90e-04
2025-05-25 12:18:59,894 - INFO - Step 1146 | Batch 90/176 | Train Loss: 1.1728 | LR: 3.90e-04
2025-05-25 12:19:04,752 - INFO - Step 1156 | Batch 100/176 | Train Loss: 1.3297 | LR: 3.90e-04
2025-05-25 12:19:09,440 - INFO - Step 1166 | Batch 110/176 | Train Loss: 0.5385 | LR: 3.90e-04
2025-05-25 12:19:14,838 - INFO - Step 1176 | Batch 120/176 | Train Loss: 2.0314 | LR: 3.90e-04
2025-05-25 12:19:20,334 - INFO - Step 1186 | Batch 130/176 | Train Loss: 2.7378 | LR: 3.90e-04
2025-05-25 12:19:26,348 - INFO - Step 1196 | Batch 140/176 | Train Loss: 0.0413 | LR: 3.90e-04
2025-05-25 12:19:31,414 - INFO - Step 1206 | Batch 150/176 | Train Loss: 0.4898 | LR: 3.90e-04
2025-05-25 12:19:36,278 - INFO - Step 1216 | Batch 160/176 | Train Loss: 0.5739 | LR: 3.90e-04
2025-05-25 12:19:41,318 - INFO - Step 1226 | Batch 170/176 | Train Loss: 1.4361 | LR: 3.90e-04
2025-05-25 12:19:53,253 - INFO - Validation Loss: 1.3238
2025-05-25 12:19:53,321 - INFO - Epoch 7/50 Summary:
2025-05-25 12:19:53,321 - INFO - Train Loss: 1.1214 | Val Loss: 1.0982 | Best Val Loss: 0.9903
2025-05-25 12:19:53,321 - INFO - --------------------------------------------------
2025-05-25 12:19:53,324 - INFO - Epoch 8/50 | Learning Rate: 3.85e-04
2025-05-25 12:19:58,514 - INFO - Step 1242 | Batch 10/176 | Train Loss: 1.2447 | LR: 3.85e-04
2025-05-25 12:20:03,737 - INFO - Step 1252 | Batch 20/176 | Train Loss: 1.3570 | LR: 3.85e-04
2025-05-25 12:20:08,678 - INFO - Step 1262 | Batch 30/176 | Train Loss: 2.5720 | LR: 3.85e-04
2025-05-25 12:20:13,970 - INFO - Step 1272 | Batch 40/176 | Train Loss: 0.6254 | LR: 3.85e-04
2025-05-25 12:20:19,148 - INFO - Step 1282 | Batch 50/176 | Train Loss: 0.0975 | LR: 3.85e-04
2025-05-25 12:20:25,664 - INFO - Step 1292 | Batch 60/176 | Train Loss: 0.9472 | LR: 3.85e-04
2025-05-25 12:20:30,527 - INFO - Step 1302 | Batch 70/176 | Train Loss: 1.3187 | LR: 3.85e-04
2025-05-25 12:20:35,722 - INFO - Step 1312 | Batch 80/176 | Train Loss: 1.4468 | LR: 3.85e-04
2025-05-25 12:20:40,926 - INFO - Step 1322 | Batch 90/176 | Train Loss: 1.3087 | LR: 3.85e-04
2025-05-25 12:20:46,106 - INFO - Step 1332 | Batch 100/176 | Train Loss: 1.1219 | LR: 3.85e-04
2025-05-25 12:20:51,091 - INFO - Step 1342 | Batch 110/176 | Train Loss: 0.1403 | LR: 3.85e-04
2025-05-25 12:20:56,268 - INFO - Step 1352 | Batch 120/176 | Train Loss: 2.9472 | LR: 3.85e-04
2025-05-25 12:21:01,521 - INFO - Step 1362 | Batch 130/176 | Train Loss: 0.6602 | LR: 3.85e-04
2025-05-25 12:21:06,928 - INFO - Step 1372 | Batch 140/176 | Train Loss: 0.1724 | LR: 3.85e-04
2025-05-25 12:21:12,150 - INFO - Step 1382 | Batch 150/176 | Train Loss: 1.0392 | LR: 3.85e-04
2025-05-25 12:21:17,341 - INFO - Step 1392 | Batch 160/176 | Train Loss: 0.1574 | LR: 3.85e-04
2025-05-25 12:21:23,779 - INFO - Step 1402 | Batch 170/176 | Train Loss: 1.7130 | LR: 3.85e-04
2025-05-25 12:21:35,619 - INFO - Validation Loss: 0.9471
2025-05-25 12:21:35,847 - INFO - Epoch 8/50 Summary:
2025-05-25 12:21:35,848 - INFO - Train Loss: 0.9897 | Val Loss: 1.0474 | Best Val Loss: 0.9903
2025-05-25 12:21:35,848 - INFO - --------------------------------------------------
2025-05-25 12:21:35,850 - INFO - Epoch 9/50 | Learning Rate: 3.80e-04
2025-05-25 12:21:40,818 - INFO - Step 1418 | Batch 10/176 | Train Loss: 0.5049 | LR: 3.80e-04
2025-05-25 12:21:45,738 - INFO - Step 1428 | Batch 20/176 | Train Loss: 1.9980 | LR: 3.80e-04
2025-05-25 12:21:50,444 - INFO - Step 1438 | Batch 30/176 | Train Loss: 0.8176 | LR: 3.80e-04
2025-05-25 12:21:55,662 - INFO - Step 1448 | Batch 40/176 | Train Loss: 1.1970 | LR: 3.80e-04
2025-05-25 12:22:00,515 - INFO - Step 1458 | Batch 50/176 | Train Loss: 1.5714 | LR: 3.80e-04
2025-05-25 12:22:05,515 - INFO - Step 1468 | Batch 60/176 | Train Loss: 1.1763 | LR: 3.80e-04
2025-05-25 12:22:10,756 - INFO - Step 1478 | Batch 70/176 | Train Loss: 3.2427 | LR: 3.80e-04
2025-05-25 12:22:15,808 - INFO - Step 1488 | Batch 80/176 | Train Loss: 1.9201 | LR: 3.80e-04
2025-05-25 12:22:22,044 - INFO - Step 1498 | Batch 90/176 | Train Loss: 3.4995 | LR: 3.80e-04
2025-05-25 12:22:27,619 - INFO - Step 1508 | Batch 100/176 | Train Loss: 1.6179 | LR: 3.80e-04
2025-05-25 12:22:32,785 - INFO - Step 1518 | Batch 110/176 | Train Loss: 0.6827 | LR: 3.80e-04
2025-05-25 12:22:37,885 - INFO - Step 1528 | Batch 120/176 | Train Loss: 1.8572 | LR: 3.80e-04
2025-05-25 12:22:43,040 - INFO - Step 1538 | Batch 130/176 | Train Loss: 1.2308 | LR: 3.80e-04
2025-05-25 12:22:48,362 - INFO - Step 1548 | Batch 140/176 | Train Loss: 1.4177 | LR: 3.80e-04
2025-05-25 12:22:53,321 - INFO - Step 1558 | Batch 150/176 | Train Loss: 1.1872 | LR: 3.80e-04
2025-05-25 12:22:58,310 - INFO - Step 1568 | Batch 160/176 | Train Loss: 0.8315 | LR: 3.80e-04
2025-05-25 12:23:03,499 - INFO - Step 1578 | Batch 170/176 | Train Loss: 1.8743 | LR: 3.80e-04
2025-05-25 12:23:15,121 - INFO - Validation Loss: 0.9736
2025-05-25 12:23:15,129 - INFO - Epoch 9/50 Summary:
2025-05-25 12:23:15,129 - INFO - Train Loss: 1.1839 | Val Loss: 1.0448 | Best Val Loss: 0.9903
2025-05-25 12:23:15,129 - INFO - --------------------------------------------------
2025-05-25 12:23:15,131 - INFO - Epoch 10/50 | Learning Rate: 3.74e-04
2025-05-25 12:23:21,069 - INFO - Step 1594 | Batch 10/176 | Train Loss: 1.8526 | LR: 3.74e-04
2025-05-25 12:23:27,427 - INFO - Step 1604 | Batch 20/176 | Train Loss: 0.1490 | LR: 3.74e-04
2025-05-25 12:23:32,829 - INFO - Step 1614 | Batch 30/176 | Train Loss: 1.9578 | LR: 3.74e-04
2025-05-25 12:23:38,185 - INFO - Step 1624 | Batch 40/176 | Train Loss: 2.1027 | LR: 3.74e-04
2025-05-25 12:23:43,296 - INFO - Step 1634 | Batch 50/176 | Train Loss: 0.9862 | LR: 3.74e-04
2025-05-25 12:23:48,476 - INFO - Step 1644 | Batch 60/176 | Train Loss: 0.9122 | LR: 3.74e-04
2025-05-25 12:23:53,489 - INFO - Step 1654 | Batch 70/176 | Train Loss: 1.3476 | LR: 3.74e-04
2025-05-25 12:23:58,777 - INFO - Step 1664 | Batch 80/176 | Train Loss: 0.7206 | LR: 3.74e-04
2025-05-25 12:24:03,921 - INFO - Step 1674 | Batch 90/176 | Train Loss: 0.6247 | LR: 3.74e-04
2025-05-25 12:24:08,881 - INFO - Step 1684 | Batch 100/176 | Train Loss: 0.2853 | LR: 3.74e-04
2025-05-25 12:24:14,183 - INFO - Step 1694 | Batch 110/176 | Train Loss: 0.7257 | LR: 3.74e-04
2025-05-25 12:24:19,927 - INFO - Step 1704 | Batch 120/176 | Train Loss: 0.7378 | LR: 3.74e-04
2025-05-25 12:24:26,118 - INFO - Step 1714 | Batch 130/176 | Train Loss: 1.1176 | LR: 3.74e-04
2025-05-25 12:24:31,423 - INFO - Step 1724 | Batch 140/176 | Train Loss: 0.7999 | LR: 3.74e-04
2025-05-25 12:24:36,551 - INFO - Step 1734 | Batch 150/176 | Train Loss: 0.9605 | LR: 3.74e-04
2025-05-25 12:24:41,682 - INFO - Step 1744 | Batch 160/176 | Train Loss: 0.4455 | LR: 3.74e-04
2025-05-25 12:24:46,993 - INFO - Step 1754 | Batch 170/176 | Train Loss: 0.9018 | LR: 3.74e-04
2025-05-25 12:24:59,511 - INFO - Validation Loss: 1.0689
2025-05-25 12:24:59,530 - INFO - Epoch 10/50 Summary:
2025-05-25 12:24:59,530 - INFO - Train Loss: 1.0544 | Val Loss: 1.1421 | Best Val Loss: 0.9903
2025-05-25 12:24:59,530 - INFO - --------------------------------------------------
2025-05-25 12:24:59,533 - INFO - Epoch 11/50 | Learning Rate: 3.68e-04
2025-05-25 12:25:04,703 - INFO - Step 1770 | Batch 10/176 | Train Loss: 1.6339 | LR: 3.68e-04
2025-05-25 12:25:09,922 - INFO - Step 1780 | Batch 20/176 | Train Loss: 2.8803 | LR: 3.68e-04
2025-05-25 12:25:15,018 - INFO - Step 1790 | Batch 30/176 | Train Loss: 0.1018 | LR: 3.68e-04
2025-05-25 12:25:20,775 - INFO - Step 1800 | Batch 40/176 | Train Loss: 1.6306 | LR: 3.68e-04
2025-05-25 12:25:26,913 - INFO - Step 1810 | Batch 50/176 | Train Loss: 1.3697 | LR: 3.68e-04
2025-05-25 12:25:32,060 - INFO - Step 1820 | Batch 60/176 | Train Loss: 1.9751 | LR: 3.68e-04
2025-05-25 12:25:36,921 - INFO - Step 1830 | Batch 70/176 | Train Loss: 1.0219 | LR: 3.68e-04
2025-05-25 12:25:42,238 - INFO - Step 1840 | Batch 80/176 | Train Loss: 2.5992 | LR: 3.68e-04
2025-05-25 12:25:47,284 - INFO - Step 1850 | Batch 90/176 | Train Loss: 1.3639 | LR: 3.68e-04
2025-05-25 12:25:52,342 - INFO - Step 1860 | Batch 100/176 | Train Loss: 0.1634 | LR: 3.68e-04
2025-05-25 12:25:57,387 - INFO - Step 1870 | Batch 110/176 | Train Loss: 0.0228 | LR: 3.68e-04
2025-05-25 12:26:02,527 - INFO - Step 1880 | Batch 120/176 | Train Loss: 0.4210 | LR: 3.68e-04
2025-05-25 12:26:07,797 - INFO - Step 1890 | Batch 130/176 | Train Loss: 0.7865 | LR: 3.68e-04
2025-05-25 12:26:13,041 - INFO - Step 1900 | Batch 140/176 | Train Loss: 1.1087 | LR: 3.68e-04
2025-05-25 12:26:18,273 - INFO - Step 1910 | Batch 150/176 | Train Loss: 0.8938 | LR: 3.68e-04
2025-05-25 12:26:24,965 - INFO - Step 1920 | Batch 160/176 | Train Loss: 0.2818 | LR: 3.68e-04
2025-05-25 12:26:29,823 - INFO - Step 1930 | Batch 170/176 | Train Loss: 0.0934 | LR: 3.68e-04
2025-05-25 12:26:41,426 - INFO - Validation Loss: 1.8259
2025-05-25 12:26:41,434 - INFO - Epoch 11/50 Summary:
2025-05-25 12:26:41,434 - INFO - Train Loss: 1.0537 | Val Loss: 1.4360 | Best Val Loss: 0.9903
2025-05-25 12:26:41,434 - INFO - --------------------------------------------------
2025-05-25 12:26:41,436 - INFO - Epoch 12/50 | Learning Rate: 3.60e-04
2025-05-25 12:26:46,491 - INFO - Step 1946 | Batch 10/176 | Train Loss: 0.2604 | LR: 3.60e-04
2025-05-25 12:26:51,496 - INFO - Step 1956 | Batch 20/176 | Train Loss: 0.9141 | LR: 3.60e-04
2025-05-25 12:26:56,619 - INFO - Step 1966 | Batch 30/176 | Train Loss: 1.3309 | LR: 3.60e-04
2025-05-25 12:27:01,711 - INFO - Step 1976 | Batch 40/176 | Train Loss: 0.4045 | LR: 3.60e-04
2025-05-25 12:27:06,323 - INFO - Step 1986 | Batch 50/176 | Train Loss: 0.7040 | LR: 3.60e-04
2025-05-25 12:27:10,470 - INFO - Step 1996 | Batch 60/176 | Train Loss: 3.5022 | LR: 3.60e-04
2025-05-25 12:27:14,904 - INFO - Step 2006 | Batch 70/176 | Train Loss: 1.2646 | LR: 3.60e-04
2025-05-25 12:27:20,427 - INFO - Step 2016 | Batch 80/176 | Train Loss: 1.5705 | LR: 3.60e-04
2025-05-25 12:27:26,598 - INFO - Step 2026 | Batch 90/176 | Train Loss: 0.0505 | LR: 3.60e-04
2025-05-25 12:27:31,563 - INFO - Step 2036 | Batch 100/176 | Train Loss: 0.3172 | LR: 3.60e-04
2025-05-25 12:27:36,463 - INFO - Step 2046 | Batch 110/176 | Train Loss: 2.7251 | LR: 3.60e-04
2025-05-25 12:27:41,916 - INFO - Step 2056 | Batch 120/176 | Train Loss: 1.1286 | LR: 3.60e-04
2025-05-25 12:27:46,783 - INFO - Step 2066 | Batch 130/176 | Train Loss: 2.0783 | LR: 3.60e-04
2025-05-25 12:27:51,778 - INFO - Step 2076 | Batch 140/176 | Train Loss: 1.0630 | LR: 3.60e-04
2025-05-25 12:27:57,119 - INFO - Step 2086 | Batch 150/176 | Train Loss: 2.5259 | LR: 3.60e-04
2025-05-25 12:28:02,284 - INFO - Step 2096 | Batch 160/176 | Train Loss: 0.7818 | LR: 3.60e-04
2025-05-25 12:28:07,582 - INFO - Step 2106 | Batch 170/176 | Train Loss: 3.3472 | LR: 3.60e-04
2025-05-25 12:28:18,729 - INFO - Validation Loss: 0.8866
2025-05-25 12:28:18,884 - INFO - Epoch 12/50 Summary:
2025-05-25 12:28:18,885 - INFO - Train Loss: 1.0211 | Val Loss: 1.1834 | Best Val Loss: 0.9903
2025-05-25 12:28:18,885 - INFO - --------------------------------------------------
2025-05-25 12:28:18,887 - INFO - Epoch 13/50 | Learning Rate: 3.52e-04
2025-05-25 12:28:25,364 - INFO - Step 2122 | Batch 10/176 | Train Loss: 2.3933 | LR: 3.52e-04
2025-05-25 12:28:30,589 - INFO - Step 2132 | Batch 20/176 | Train Loss: 0.5105 | LR: 3.52e-04
2025-05-25 12:28:35,871 - INFO - Step 2142 | Batch 30/176 | Train Loss: 0.9402 | LR: 3.52e-04
2025-05-25 12:28:41,109 - INFO - Step 2152 | Batch 40/176 | Train Loss: 1.8309 | LR: 3.52e-04
2025-05-25 12:28:46,297 - INFO - Step 2162 | Batch 50/176 | Train Loss: 0.2972 | LR: 3.52e-04
2025-05-25 12:28:51,557 - INFO - Step 2172 | Batch 60/176 | Train Loss: 1.5614 | LR: 3.52e-04
2025-05-25 12:28:56,587 - INFO - Step 2182 | Batch 70/176 | Train Loss: 0.7595 | LR: 3.52e-04
2025-05-25 12:29:01,639 - INFO - Step 2192 | Batch 80/176 | Train Loss: 1.3857 | LR: 3.52e-04
2025-05-25 12:29:06,501 - INFO - Step 2202 | Batch 90/176 | Train Loss: 0.6638 | LR: 3.52e-04
2025-05-25 12:29:11,406 - INFO - Step 2212 | Batch 100/176 | Train Loss: 0.1057 | LR: 3.52e-04
2025-05-25 12:29:16,634 - INFO - Step 2222 | Batch 110/176 | Train Loss: 0.1790 | LR: 3.52e-04
2025-05-25 12:29:22,679 - INFO - Step 2232 | Batch 120/176 | Train Loss: 1.3777 | LR: 3.52e-04
2025-05-25 12:29:28,104 - INFO - Step 2242 | Batch 130/176 | Train Loss: 0.3004 | LR: 3.52e-04
2025-05-25 12:29:33,368 - INFO - Step 2252 | Batch 140/176 | Train Loss: 0.2657 | LR: 3.52e-04
2025-05-25 12:29:38,564 - INFO - Step 2262 | Batch 150/176 | Train Loss: 0.0096 | LR: 3.52e-04
2025-05-25 12:29:43,241 - INFO - Step 2272 | Batch 160/176 | Train Loss: 2.8909 | LR: 3.52e-04
2025-05-25 12:29:48,349 - INFO - Step 2282 | Batch 170/176 | Train Loss: 0.4021 | LR: 3.52e-04
2025-05-25 12:29:59,606 - INFO - Validation Loss: 2.8125
2025-05-25 12:29:59,614 - INFO - Epoch 13/50 Summary:
2025-05-25 12:29:59,615 - INFO - Train Loss: 0.9870 | Val Loss: 1.7096 | Best Val Loss: 0.9903
2025-05-25 12:29:59,615 - INFO - --------------------------------------------------
2025-05-25 12:29:59,617 - INFO - Epoch 14/50 | Learning Rate: 3.44e-04
2025-05-25 12:30:05,070 - INFO - Step 2298 | Batch 10/176 | Train Loss: 0.2009 | LR: 3.44e-04
2025-05-25 12:30:10,734 - INFO - Step 2308 | Batch 20/176 | Train Loss: 1.5790 | LR: 3.44e-04
2025-05-25 12:30:17,601 - INFO - Step 2318 | Batch 30/176 | Train Loss: 0.8822 | LR: 3.44e-04
2025-05-25 12:30:24,675 - INFO - Step 2328 | Batch 40/176 | Train Loss: 0.9001 | LR: 3.44e-04
2025-05-25 12:30:29,636 - INFO - Step 2338 | Batch 50/176 | Train Loss: 0.7275 | LR: 3.44e-04
2025-05-25 12:30:34,469 - INFO - Step 2348 | Batch 60/176 | Train Loss: 1.0699 | LR: 3.44e-04
2025-05-25 12:30:39,629 - INFO - Step 2358 | Batch 70/176 | Train Loss: 1.5801 | LR: 3.44e-04
2025-05-25 12:30:44,677 - INFO - Step 2368 | Batch 80/176 | Train Loss: 2.8318 | LR: 3.44e-04
2025-05-25 12:30:49,776 - INFO - Step 2378 | Batch 90/176 | Train Loss: 0.3453 | LR: 3.44e-04
2025-05-25 12:30:54,652 - INFO - Step 2388 | Batch 100/176 | Train Loss: 2.2630 | LR: 3.44e-04
2025-05-25 12:30:59,692 - INFO - Step 2398 | Batch 110/176 | Train Loss: 2.4985 | LR: 3.44e-04
2025-05-25 12:31:04,932 - INFO - Step 2408 | Batch 120/176 | Train Loss: 2.6988 | LR: 3.44e-04
2025-05-25 12:31:10,017 - INFO - Step 2418 | Batch 130/176 | Train Loss: 0.9658 | LR: 3.44e-04
2025-05-25 12:31:15,200 - INFO - Step 2428 | Batch 140/176 | Train Loss: 2.1625 | LR: 3.44e-04
2025-05-25 12:31:21,116 - INFO - Step 2438 | Batch 150/176 | Train Loss: 0.3882 | LR: 3.44e-04
2025-05-25 12:31:27,104 - INFO - Step 2448 | Batch 160/176 | Train Loss: 0.0128 | LR: 3.44e-04
2025-05-25 12:31:32,331 - INFO - Step 2458 | Batch 170/176 | Train Loss: 0.9210 | LR: 3.44e-04
2025-05-25 12:31:42,235 - INFO - Validation Loss: 73.0697
2025-05-25 12:31:42,524 - INFO - Epoch 14/50 Summary:
2025-05-25 12:31:42,526 - INFO - Train Loss: 1.0875 | Val Loss: 19.4484 | Best Val Loss: 0.9903
2025-05-25 12:31:42,526 - INFO - --------------------------------------------------
2025-05-25 12:31:42,528 - INFO - Epoch 15/50 | Learning Rate: 3.34e-04
2025-05-25 12:31:47,725 - INFO - Step 2474 | Batch 10/176 | Train Loss: 0.1281 | LR: 3.34e-04
2025-05-25 12:31:52,914 - INFO - Step 2484 | Batch 20/176 | Train Loss: 0.4538 | LR: 3.34e-04
2025-05-25 12:31:58,071 - INFO - Step 2494 | Batch 30/176 | Train Loss: 0.0086 | LR: 3.34e-04
2025-05-25 12:32:02,891 - INFO - Step 2504 | Batch 40/176 | Train Loss: 0.7351 | LR: 3.34e-04
2025-05-25 12:32:07,794 - INFO - Step 2514 | Batch 50/176 | Train Loss: 0.7417 | LR: 3.34e-04
2025-05-25 12:32:12,492 - INFO - Step 2524 | Batch 60/176 | Train Loss: 1.7956 | LR: 3.34e-04
2025-05-25 12:32:17,677 - INFO - Step 2534 | Batch 70/176 | Train Loss: 0.7198 | LR: 3.34e-04
2025-05-25 12:32:24,187 - INFO - Step 2544 | Batch 80/176 | Train Loss: 0.0715 | LR: 3.34e-04
2025-05-25 12:32:29,144 - INFO - Step 2554 | Batch 90/176 | Train Loss: 2.2338 | LR: 3.34e-04
2025-05-25 12:32:34,169 - INFO - Step 2564 | Batch 100/176 | Train Loss: 0.5109 | LR: 3.34e-04
2025-05-25 12:32:39,349 - INFO - Step 2574 | Batch 110/176 | Train Loss: 0.8860 | LR: 3.34e-04
2025-05-25 12:32:44,506 - INFO - Step 2584 | Batch 120/176 | Train Loss: 1.8903 | LR: 3.34e-04
2025-05-25 12:32:49,696 - INFO - Step 2594 | Batch 130/176 | Train Loss: 1.2010 | LR: 3.34e-04
2025-05-25 12:32:55,093 - INFO - Step 2604 | Batch 140/176 | Train Loss: 1.2795 | LR: 3.34e-04
2025-05-25 12:33:00,040 - INFO - Step 2614 | Batch 150/176 | Train Loss: 0.1363 | LR: 3.34e-04
2025-05-25 12:33:05,356 - INFO - Step 2624 | Batch 160/176 | Train Loss: 0.6290 | LR: 3.34e-04
2025-05-25 12:33:10,455 - INFO - Step 2634 | Batch 170/176 | Train Loss: 1.6040 | LR: 3.34e-04
2025-05-25 12:33:24,468 - INFO - Validation Loss: 226.6405
2025-05-25 12:33:24,599 - INFO - Epoch 15/50 Summary:
2025-05-25 12:33:24,599 - INFO - Train Loss: 1.2215 | Val Loss: 57.5595 | Best Val Loss: 0.9903
2025-05-25 12:33:24,599 - INFO - --------------------------------------------------
2025-05-25 12:33:24,602 - INFO - Epoch 16/50 | Learning Rate: 3.25e-04
2025-05-25 12:33:29,690 - INFO - Step 2650 | Batch 10/176 | Train Loss: 0.3952 | LR: 3.25e-04
2025-05-25 12:33:34,553 - INFO - Step 2660 | Batch 20/176 | Train Loss: 0.7871 | LR: 3.25e-04
2025-05-25 12:33:40,064 - INFO - Step 2670 | Batch 30/176 | Train Loss: 1.9256 | LR: 3.25e-04
2025-05-25 12:33:45,169 - INFO - Step 2680 | Batch 40/176 | Train Loss: 0.6767 | LR: 3.25e-04
2025-05-25 12:33:50,093 - INFO - Step 2690 | Batch 50/176 | Train Loss: 0.2193 | LR: 3.25e-04
2025-05-25 12:33:55,162 - INFO - Step 2700 | Batch 60/176 | Train Loss: 1.3000 | LR: 3.25e-04
2025-05-25 12:34:00,300 - INFO - Step 2710 | Batch 70/176 | Train Loss: 0.9984 | LR: 3.25e-04
2025-05-25 12:34:05,305 - INFO - Step 2720 | Batch 80/176 | Train Loss: 0.2838 | LR: 3.25e-04
2025-05-25 12:34:10,208 - INFO - Step 2730 | Batch 90/176 | Train Loss: 1.1592 | LR: 3.25e-04
2025-05-25 12:34:15,269 - INFO - Step 2740 | Batch 100/176 | Train Loss: 1.0923 | LR: 3.25e-04
2025-05-25 12:34:21,290 - INFO - Step 2750 | Batch 110/176 | Train Loss: 0.9209 | LR: 3.25e-04
2025-05-25 12:34:27,093 - INFO - Step 2760 | Batch 120/176 | Train Loss: 0.8428 | LR: 3.25e-04
2025-05-25 12:34:32,182 - INFO - Step 2770 | Batch 130/176 | Train Loss: 0.5709 | LR: 3.25e-04
2025-05-25 12:34:37,206 - INFO - Step 2780 | Batch 140/176 | Train Loss: 0.3181 | LR: 3.25e-04
2025-05-25 12:34:42,271 - INFO - Step 2790 | Batch 150/176 | Train Loss: 0.6099 | LR: 3.25e-04
2025-05-25 12:34:47,533 - INFO - Step 2800 | Batch 160/176 | Train Loss: 0.8508 | LR: 3.25e-04
2025-05-25 12:34:52,452 - INFO - Step 2810 | Batch 170/176 | Train Loss: 0.9164 | LR: 3.25e-04
2025-05-25 12:35:03,894 - INFO - Validation Loss: 1.6093
2025-05-25 12:35:03,903 - INFO - Epoch 16/50 Summary:
2025-05-25 12:35:03,903 - INFO - Train Loss: 1.0370 | Val Loss: 1.3240 | Best Val Loss: 0.9903
2025-05-25 12:35:03,903 - INFO - --------------------------------------------------
2025-05-25 12:35:03,906 - INFO - Epoch 17/50 | Learning Rate: 3.14e-04
2025-05-25 12:35:08,911 - INFO - Step 2826 | Batch 10/176 | Train Loss: 1.0834 | LR: 3.14e-04
2025-05-25 12:35:14,056 - INFO - Step 2836 | Batch 20/176 | Train Loss: 1.0515 | LR: 3.14e-04
2025-05-25 12:35:19,436 - INFO - Step 2846 | Batch 30/176 | Train Loss: 0.5286 | LR: 3.14e-04
2025-05-25 12:35:25,984 - INFO - Step 2856 | Batch 40/176 | Train Loss: 1.5082 | LR: 3.14e-04
2025-05-25 12:35:31,257 - INFO - Step 2866 | Batch 50/176 | Train Loss: 0.0157 | LR: 3.14e-04
2025-05-25 12:35:36,543 - INFO - Step 2876 | Batch 60/176 | Train Loss: 0.3991 | LR: 3.14e-04
2025-05-25 12:35:41,595 - INFO - Step 2886 | Batch 70/176 | Train Loss: 0.2136 | LR: 3.14e-04
2025-05-25 12:35:46,718 - INFO - Step 2896 | Batch 80/176 | Train Loss: 0.6932 | LR: 3.14e-04
2025-05-25 12:35:51,735 - INFO - Step 2906 | Batch 90/176 | Train Loss: 1.3320 | LR: 3.14e-04
2025-05-25 12:35:56,779 - INFO - Step 2916 | Batch 100/176 | Train Loss: 0.2531 | LR: 3.14e-04
2025-05-25 12:36:02,265 - INFO - Step 2926 | Batch 110/176 | Train Loss: 0.5093 | LR: 3.14e-04
2025-05-25 12:36:07,867 - INFO - Step 2936 | Batch 120/176 | Train Loss: 0.8672 | LR: 3.14e-04
2025-05-25 12:36:13,964 - INFO - Step 2946 | Batch 130/176 | Train Loss: 0.0144 | LR: 3.14e-04
2025-05-25 12:36:19,843 - INFO - Step 2956 | Batch 140/176 | Train Loss: 0.2969 | LR: 3.14e-04
2025-05-25 12:36:26,500 - INFO - Step 2966 | Batch 150/176 | Train Loss: 0.2507 | LR: 3.14e-04
2025-05-25 12:36:31,643 - INFO - Step 2976 | Batch 160/176 | Train Loss: 0.1324 | LR: 3.14e-04
2025-05-25 12:36:36,512 - INFO - Step 2986 | Batch 170/176 | Train Loss: 2.0343 | LR: 3.14e-04
2025-05-25 12:36:47,581 - INFO - Validation Loss: 0.9166
2025-05-25 12:36:47,590 - INFO - Epoch 17/50 Summary:
2025-05-25 12:36:47,590 - INFO - Train Loss: 1.0472 | Val Loss: 1.2119 | Best Val Loss: 0.9903
2025-05-25 12:36:47,590 - INFO - --------------------------------------------------
2025-05-25 12:36:47,593 - INFO - Epoch 18/50 | Learning Rate: 3.04e-04
2025-05-25 12:36:52,777 - INFO - Step 3002 | Batch 10/176 | Train Loss: 2.7442 | LR: 3.04e-04
2025-05-25 12:36:57,697 - INFO - Step 3012 | Batch 20/176 | Train Loss: 0.5571 | LR: 3.04e-04
2025-05-25 12:37:02,772 - INFO - Step 3022 | Batch 30/176 | Train Loss: 2.1461 | LR: 3.04e-04
2025-05-25 12:37:07,580 - INFO - Step 3032 | Batch 40/176 | Train Loss: 3.3336 | LR: 3.04e-04
2025-05-25 12:37:12,663 - INFO - Step 3042 | Batch 50/176 | Train Loss: 0.7259 | LR: 3.04e-04
2025-05-25 12:37:17,850 - INFO - Step 3052 | Batch 60/176 | Train Loss: 1.4971 | LR: 3.04e-04
2025-05-25 12:37:24,249 - INFO - Step 3062 | Batch 70/176 | Train Loss: 0.3120 | LR: 3.04e-04
2025-05-25 12:37:29,340 - INFO - Step 3072 | Batch 80/176 | Train Loss: 0.1378 | LR: 3.04e-04
2025-05-25 12:37:34,445 - INFO - Step 3082 | Batch 90/176 | Train Loss: 0.3851 | LR: 3.04e-04
2025-05-25 12:37:39,393 - INFO - Step 3092 | Batch 100/176 | Train Loss: 1.1203 | LR: 3.04e-04
2025-05-25 12:37:44,595 - INFO - Step 3102 | Batch 110/176 | Train Loss: 1.1711 | LR: 3.04e-04
2025-05-25 12:37:49,538 - INFO - Step 3112 | Batch 120/176 | Train Loss: 1.2341 | LR: 3.04e-04
2025-05-25 12:37:54,546 - INFO - Step 3122 | Batch 130/176 | Train Loss: 0.7707 | LR: 3.04e-04
2025-05-25 12:37:59,580 - INFO - Step 3132 | Batch 140/176 | Train Loss: 1.5656 | LR: 3.04e-04
2025-05-25 12:38:04,824 - INFO - Step 3142 | Batch 150/176 | Train Loss: 2.3824 | LR: 3.04e-04
2025-05-25 12:38:09,987 - INFO - Step 3152 | Batch 160/176 | Train Loss: 1.9032 | LR: 3.04e-04
2025-05-25 12:38:15,130 - INFO - Step 3162 | Batch 170/176 | Train Loss: 0.8864 | LR: 3.04e-04
2025-05-25 12:38:29,244 - INFO - Validation Loss: 2.0085
2025-05-25 12:38:29,439 - INFO - Epoch 18/50 Summary:
2025-05-25 12:38:29,440 - INFO - Train Loss: 1.0859 | Val Loss: 1.5353 | Best Val Loss: 0.9903
2025-05-25 12:38:29,440 - INFO - --------------------------------------------------
2025-05-25 12:38:29,443 - INFO - Epoch 19/50 | Learning Rate: 2.93e-04
2025-05-25 12:38:34,666 - INFO - Step 3178 | Batch 10/176 | Train Loss: 1.8011 | LR: 2.93e-04
2025-05-25 12:38:39,936 - INFO - Step 3188 | Batch 20/176 | Train Loss: 1.7864 | LR: 2.93e-04
2025-05-25 12:38:44,892 - INFO - Step 3198 | Batch 30/176 | Train Loss: 2.1979 | LR: 2.93e-04
2025-05-25 12:38:49,906 - INFO - Step 3208 | Batch 40/176 | Train Loss: 0.4850 | LR: 2.93e-04
2025-05-25 12:38:55,115 - INFO - Step 3218 | Batch 50/176 | Train Loss: 0.9707 | LR: 2.93e-04
2025-05-25 12:39:00,499 - INFO - Step 3228 | Batch 60/176 | Train Loss: 0.9269 | LR: 2.93e-04
2025-05-25 12:39:05,797 - INFO - Step 3238 | Batch 70/176 | Train Loss: 2.2144 | LR: 2.93e-04
2025-05-25 12:39:10,849 - INFO - Step 3248 | Batch 80/176 | Train Loss: 0.3661 | LR: 2.93e-04
2025-05-25 12:39:16,050 - INFO - Step 3258 | Batch 90/176 | Train Loss: 0.8510 | LR: 2.93e-04
2025-05-25 12:39:22,198 - INFO - Step 3268 | Batch 100/176 | Train Loss: 1.4693 | LR: 2.93e-04
2025-05-25 12:39:27,840 - INFO - Step 3278 | Batch 110/176 | Train Loss: 1.2000 | LR: 2.93e-04
2025-05-25 12:39:32,970 - INFO - Step 3288 | Batch 120/176 | Train Loss: 0.2343 | LR: 2.93e-04
2025-05-25 12:39:37,730 - INFO - Step 3298 | Batch 130/176 | Train Loss: 0.6151 | LR: 2.93e-04
2025-05-25 12:39:42,426 - INFO - Step 3308 | Batch 140/176 | Train Loss: 1.9223 | LR: 2.93e-04
2025-05-25 12:39:46,305 - INFO - Step 3318 | Batch 150/176 | Train Loss: 0.2993 | LR: 2.93e-04
2025-05-25 12:39:50,452 - INFO - Step 3328 | Batch 160/176 | Train Loss: 0.1604 | LR: 2.93e-04
2025-05-25 12:39:54,208 - INFO - Step 3338 | Batch 170/176 | Train Loss: 0.8327 | LR: 2.93e-04
2025-05-25 12:40:17,131 - INFO - Validation Loss: 279.7568
2025-05-25 12:40:18,264 - INFO - Epoch 19/50 Summary:
2025-05-25 12:40:18,265 - INFO - Train Loss: 1.1233 | Val Loss: 70.9641 | Best Val Loss: 0.9903
2025-05-25 12:40:18,265 - INFO - --------------------------------------------------
2025-05-25 12:40:18,268 - INFO - Epoch 20/50 | Learning Rate: 2.81e-04
2025-05-25 12:40:29,669 - INFO - Step 3354 | Batch 10/176 | Train Loss: 1.5458 | LR: 2.81e-04
2025-05-25 12:40:38,847 - INFO - Step 3364 | Batch 20/176 | Train Loss: 0.2569 | LR: 2.81e-04
2025-05-25 12:40:48,244 - INFO - Step 3374 | Batch 30/176 | Train Loss: 0.9754 | LR: 2.81e-04
2025-05-25 12:40:57,489 - INFO - Step 3384 | Batch 40/176 | Train Loss: 0.6478 | LR: 2.81e-04
2025-05-25 12:41:06,741 - INFO - Step 3394 | Batch 50/176 | Train Loss: 1.8613 | LR: 2.81e-04
2025-05-25 12:41:16,018 - INFO - Step 3404 | Batch 60/176 | Train Loss: 2.1718 | LR: 2.81e-04
2025-05-25 12:41:26,954 - INFO - Step 3414 | Batch 70/176 | Train Loss: 0.3823 | LR: 2.81e-04
2025-05-25 12:41:36,071 - INFO - Step 3424 | Batch 80/176 | Train Loss: 0.4303 | LR: 2.81e-04
2025-05-25 12:41:45,581 - INFO - Step 3434 | Batch 90/176 | Train Loss: 1.6488 | LR: 2.81e-04
2025-05-25 12:41:55,122 - INFO - Step 3444 | Batch 100/176 | Train Loss: 1.1007 | LR: 2.81e-04
2025-05-25 12:42:04,530 - INFO - Step 3454 | Batch 110/176 | Train Loss: 2.0766 | LR: 2.81e-04
2025-05-25 12:42:14,079 - INFO - Step 3464 | Batch 120/176 | Train Loss: 0.1275 | LR: 2.81e-04
2025-05-25 12:42:25,616 - INFO - Step 3474 | Batch 130/176 | Train Loss: 2.7474 | LR: 2.81e-04
2025-05-25 12:42:35,457 - INFO - Step 3484 | Batch 140/176 | Train Loss: 0.4162 | LR: 2.81e-04
2025-05-25 12:42:44,847 - INFO - Step 3494 | Batch 150/176 | Train Loss: 1.8444 | LR: 2.81e-04
2025-05-25 12:42:53,836 - INFO - Step 3504 | Batch 160/176 | Train Loss: 0.2675 | LR: 2.81e-04
2025-05-25 12:43:03,184 - INFO - Step 3514 | Batch 170/176 | Train Loss: 0.8495 | LR: 2.81e-04
2025-05-25 12:43:40,730 - INFO - Validation Loss: 421.6343
2025-05-25 12:43:40,735 - INFO - Epoch 20/50 Summary:
2025-05-25 12:43:40,736 - INFO - Train Loss: 0.9732 | Val Loss: 106.5777 | Best Val Loss: 0.9903
2025-05-25 12:43:40,736 - INFO - --------------------------------------------------
2025-05-25 12:43:40,739 - INFO - Epoch 21/50 | Learning Rate: 2.69e-04
2025-05-25 12:43:50,062 - INFO - Step 3530 | Batch 10/176 | Train Loss: 2.1637 | LR: 2.69e-04
2025-05-25 12:43:59,289 - INFO - Step 3540 | Batch 20/176 | Train Loss: 1.4256 | LR: 2.69e-04
2025-05-25 12:44:08,774 - INFO - Step 3550 | Batch 30/176 | Train Loss: 1.4949 | LR: 2.69e-04
2025-05-25 12:44:17,973 - INFO - Step 3560 | Batch 40/176 | Train Loss: 1.1379 | LR: 2.69e-04
2025-05-25 12:44:28,925 - INFO - Step 3570 | Batch 50/176 | Train Loss: 0.7527 | LR: 2.69e-04
2025-05-25 12:44:38,095 - INFO - Step 3580 | Batch 60/176 | Train Loss: 0.0799 | LR: 2.69e-04
2025-05-25 12:44:47,334 - INFO - Step 3590 | Batch 70/176 | Train Loss: 0.1449 | LR: 2.69e-04
2025-05-25 12:44:56,695 - INFO - Step 3600 | Batch 80/176 | Train Loss: 1.5435 | LR: 2.69e-04
2025-05-25 12:45:06,376 - INFO - Step 3610 | Batch 90/176 | Train Loss: 2.1732 | LR: 2.69e-04
2025-05-25 12:45:16,425 - INFO - Step 3620 | Batch 100/176 | Train Loss: 1.8566 | LR: 2.69e-04
2025-05-25 12:45:28,049 - INFO - Step 3630 | Batch 110/176 | Train Loss: 2.7865 | LR: 2.69e-04
2025-05-25 12:45:37,336 - INFO - Step 3640 | Batch 120/176 | Train Loss: 0.8603 | LR: 2.69e-04
2025-05-25 12:45:46,837 - INFO - Step 3650 | Batch 130/176 | Train Loss: 0.9062 | LR: 2.69e-04
2025-05-25 12:45:56,355 - INFO - Step 3660 | Batch 140/176 | Train Loss: 0.0279 | LR: 2.69e-04
2025-05-25 12:46:05,739 - INFO - Step 3670 | Batch 150/176 | Train Loss: 1.3284 | LR: 2.69e-04
2025-05-25 12:46:15,376 - INFO - Step 3680 | Batch 160/176 | Train Loss: 0.9222 | LR: 2.69e-04
2025-05-25 12:46:26,699 - INFO - Step 3690 | Batch 170/176 | Train Loss: 0.1325 | LR: 2.69e-04
2025-05-25 12:47:02,479 - INFO - Validation Loss: 12.3620
2025-05-25 12:47:02,494 - INFO - Epoch 21/50 Summary:
2025-05-25 12:47:02,494 - INFO - Train Loss: 1.0818 | Val Loss: 4.3077 | Best Val Loss: 0.9903
2025-05-25 12:47:02,495 - INFO - --------------------------------------------------
2025-05-25 12:47:02,498 - INFO - Epoch 22/50 | Learning Rate: 2.57e-04
2025-05-25 12:47:11,818 - INFO - Step 3706 | Batch 10/176 | Train Loss: 0.7705 | LR: 2.57e-04
2025-05-25 12:47:22,629 - INFO - Step 3716 | Batch 20/176 | Train Loss: 0.7054 | LR: 2.57e-04
2025-05-25 12:47:32,277 - INFO - Step 3726 | Batch 30/176 | Train Loss: 1.2628 | LR: 2.57e-04
2025-05-25 12:47:41,763 - INFO - Step 3736 | Batch 40/176 | Train Loss: 0.7498 | LR: 2.57e-04
2025-05-25 12:47:51,371 - INFO - Step 3746 | Batch 50/176 | Train Loss: 1.1184 | LR: 2.57e-04
2025-05-25 12:48:00,747 - INFO - Step 3756 | Batch 60/176 | Train Loss: 0.0546 | LR: 2.57e-04
2025-05-25 12:48:10,332 - INFO - Step 3766 | Batch 70/176 | Train Loss: 1.2584 | LR: 2.57e-04
2025-05-25 12:48:20,215 - INFO - Step 3776 | Batch 80/176 | Train Loss: 0.5186 | LR: 2.57e-04
2025-05-25 12:48:31,015 - INFO - Step 3786 | Batch 90/176 | Train Loss: 1.2764 | LR: 2.57e-04
2025-05-25 12:48:40,747 - INFO - Step 3796 | Batch 100/176 | Train Loss: 2.1177 | LR: 2.57e-04
2025-05-25 12:48:50,258 - INFO - Step 3806 | Batch 110/176 | Train Loss: 0.0299 | LR: 2.57e-04
2025-05-25 12:48:59,692 - INFO - Step 3816 | Batch 120/176 | Train Loss: 0.9566 | LR: 2.57e-04
2025-05-25 12:49:09,053 - INFO - Step 3826 | Batch 130/176 | Train Loss: 2.4561 | LR: 2.57e-04
2025-05-25 12:49:18,329 - INFO - Step 3836 | Batch 140/176 | Train Loss: 1.1821 | LR: 2.57e-04
2025-05-25 12:49:29,517 - INFO - Step 3846 | Batch 150/176 | Train Loss: 0.5866 | LR: 2.57e-04
2025-05-25 12:49:40,477 - INFO - Step 3856 | Batch 160/176 | Train Loss: 0.0442 | LR: 2.57e-04
2025-05-25 12:49:51,027 - INFO - Step 3866 | Batch 170/176 | Train Loss: 0.1178 | LR: 2.57e-04
2025-05-25 12:50:32,765 - INFO - Validation Loss: 1.0239
2025-05-25 12:50:32,865 - INFO - Epoch 22/50 Summary:
2025-05-25 12:50:32,866 - INFO - Train Loss: 1.0603 | Val Loss: 1.5966 | Best Val Loss: 0.9903
2025-05-25 12:50:32,866 - INFO - --------------------------------------------------
2025-05-25 12:50:32,869 - INFO - Epoch 23/50 | Learning Rate: 2.45e-04
2025-05-25 12:50:42,231 - INFO - Step 3882 | Batch 10/176 | Train Loss: 1.4509 | LR: 2.45e-04
2025-05-25 12:50:51,633 - INFO - Step 3892 | Batch 20/176 | Train Loss: 1.2265 | LR: 2.45e-04
2025-05-25 12:51:01,025 - INFO - Step 3902 | Batch 30/176 | Train Loss: 0.8811 | LR: 2.45e-04
2025-05-25 12:51:10,481 - INFO - Step 3912 | Batch 40/176 | Train Loss: 0.1733 | LR: 2.45e-04
2025-05-25 12:51:21,028 - INFO - Step 3922 | Batch 50/176 | Train Loss: 1.1249 | LR: 2.45e-04
2025-05-25 12:51:31,195 - INFO - Step 3932 | Batch 60/176 | Train Loss: 0.5855 | LR: 2.45e-04
2025-05-25 12:51:40,867 - INFO - Step 3942 | Batch 70/176 | Train Loss: 0.8014 | LR: 2.45e-04
2025-05-25 12:51:50,616 - INFO - Step 3952 | Batch 80/176 | Train Loss: 2.2424 | LR: 2.45e-04
2025-05-25 12:52:00,108 - INFO - Step 3962 | Batch 90/176 | Train Loss: 0.1755 | LR: 2.45e-04
2025-05-25 12:52:09,514 - INFO - Step 3972 | Batch 100/176 | Train Loss: 0.4377 | LR: 2.45e-04
2025-05-25 12:52:19,547 - INFO - Step 3982 | Batch 110/176 | Train Loss: 2.0904 | LR: 2.45e-04
2025-05-25 12:52:30,116 - INFO - Step 3992 | Batch 120/176 | Train Loss: 0.0406 | LR: 2.45e-04
2025-05-25 12:52:39,372 - INFO - Step 4002 | Batch 130/176 | Train Loss: 1.4972 | LR: 2.45e-04
2025-05-25 12:52:48,790 - INFO - Step 4012 | Batch 140/176 | Train Loss: 1.0007 | LR: 2.45e-04
2025-05-25 12:52:58,098 - INFO - Step 4022 | Batch 150/176 | Train Loss: 0.5591 | LR: 2.45e-04
2025-05-25 12:53:07,565 - INFO - Step 4032 | Batch 160/176 | Train Loss: 1.8733 | LR: 2.45e-04
2025-05-25 12:53:17,246 - INFO - Step 4042 | Batch 170/176 | Train Loss: 0.0410 | LR: 2.45e-04
2025-05-25 12:53:54,143 - INFO - Validation Loss: 95.4033
2025-05-25 12:53:54,566 - INFO - Epoch 23/50 Summary:
2025-05-25 12:53:54,566 - INFO - Train Loss: 1.0953 | Val Loss: 25.2166 | Best Val Loss: 0.9903
2025-05-25 12:53:54,566 - INFO - --------------------------------------------------
2025-05-25 12:53:54,569 - INFO - Epoch 24/50 | Learning Rate: 2.32e-04
2025-05-25 12:54:04,002 - INFO - Step 4058 | Batch 10/176 | Train Loss: 0.7645 | LR: 2.32e-04
2025-05-25 12:54:13,450 - INFO - Step 4068 | Batch 20/176 | Train Loss: 0.3947 | LR: 2.32e-04
2025-05-25 12:54:24,833 - INFO - Step 4078 | Batch 30/176 | Train Loss: 0.2567 | LR: 2.32e-04
2025-05-25 12:54:33,015 - INFO - Step 4088 | Batch 40/176 | Train Loss: 1.3876 | LR: 2.32e-04
2025-05-25 12:54:42,430 - INFO - Step 4098 | Batch 50/176 | Train Loss: 2.9059 | LR: 2.32e-04
2025-05-25 12:54:51,802 - INFO - Step 4108 | Batch 60/176 | Train Loss: 0.2548 | LR: 2.32e-04
2025-05-25 12:55:01,030 - INFO - Step 4118 | Batch 70/176 | Train Loss: 0.4048 | LR: 2.32e-04
2025-05-25 12:55:10,473 - INFO - Step 4128 | Batch 80/176 | Train Loss: 0.4055 | LR: 2.32e-04
2025-05-25 12:55:20,441 - INFO - Step 4138 | Batch 90/176 | Train Loss: 1.5839 | LR: 2.32e-04
2025-05-25 12:55:30,840 - INFO - Step 4148 | Batch 100/176 | Train Loss: 1.3679 | LR: 2.32e-04
2025-05-25 12:55:40,113 - INFO - Step 4158 | Batch 110/176 | Train Loss: 1.6836 | LR: 2.32e-04
2025-05-25 12:55:49,385 - INFO - Step 4168 | Batch 120/176 | Train Loss: 0.5717 | LR: 2.32e-04
2025-05-25 12:55:58,538 - INFO - Step 4178 | Batch 130/176 | Train Loss: 0.6001 | LR: 2.32e-04
2025-05-25 12:56:02,870 - INFO - Step 4188 | Batch 140/176 | Train Loss: 1.8058 | LR: 2.32e-04
2025-05-25 12:56:06,639 - INFO - Step 4198 | Batch 150/176 | Train Loss: 1.1237 | LR: 2.32e-04
2025-05-25 12:56:15,539 - INFO - Step 4208 | Batch 160/176 | Train Loss: 1.5032 | LR: 2.32e-04
2025-05-25 12:56:26,597 - INFO - Step 4218 | Batch 170/176 | Train Loss: 0.8684 | LR: 2.32e-04
2025-05-25 12:57:00,689 - INFO - Validation Loss: 81.3244
2025-05-25 12:57:00,698 - INFO - Epoch 24/50 Summary:
2025-05-25 12:57:00,698 - INFO - Train Loss: 0.9474 | Val Loss: 21.8051 | Best Val Loss: 0.9903
2025-05-25 12:57:00,698 - INFO - --------------------------------------------------
2025-05-25 12:57:00,701 - INFO - Epoch 25/50 | Learning Rate: 2.19e-04
2025-05-25 12:57:10,031 - INFO - Step 4234 | Batch 10/176 | Train Loss: 2.2845 | LR: 2.19e-04
2025-05-25 12:57:19,784 - INFO - Step 4244 | Batch 20/176 | Train Loss: 0.8709 | LR: 2.19e-04
2025-05-25 12:57:30,721 - INFO - Step 4254 | Batch 30/176 | Train Loss: 1.5913 | LR: 2.19e-04
2025-05-25 12:57:40,074 - INFO - Step 4264 | Batch 40/176 | Train Loss: 0.3217 | LR: 2.19e-04
2025-05-25 12:57:49,320 - INFO - Step 4274 | Batch 50/176 | Train Loss: 0.1821 | LR: 2.19e-04
2025-05-25 12:57:58,197 - INFO - Step 4284 | Batch 60/176 | Train Loss: 0.7636 | LR: 2.19e-04
2025-05-25 12:58:02,245 - INFO - Step 4294 | Batch 70/176 | Train Loss: 0.7576 | LR: 2.19e-04
2025-05-25 12:58:06,186 - INFO - Step 4304 | Batch 80/176 | Train Loss: 0.1454 | LR: 2.19e-04
2025-05-25 12:58:14,986 - INFO - Step 4314 | Batch 90/176 | Train Loss: 2.0437 | LR: 2.19e-04
2025-05-25 12:58:25,937 - INFO - Step 4324 | Batch 100/176 | Train Loss: 1.4256 | LR: 2.19e-04
2025-05-25 12:58:34,452 - INFO - Step 4334 | Batch 110/176 | Train Loss: 2.4594 | LR: 2.19e-04
2025-05-25 12:58:42,952 - INFO - Step 4344 | Batch 120/176 | Train Loss: 0.6948 | LR: 2.19e-04
2025-05-25 12:58:51,254 - INFO - Step 4354 | Batch 130/176 | Train Loss: 0.8499 | LR: 2.19e-04
2025-05-25 12:58:59,360 - INFO - Step 4364 | Batch 140/176 | Train Loss: 1.8629 | LR: 2.19e-04
2025-05-25 12:59:07,760 - INFO - Step 4374 | Batch 150/176 | Train Loss: 0.5633 | LR: 2.19e-04
2025-05-25 12:59:16,154 - INFO - Step 4384 | Batch 160/176 | Train Loss: 1.2264 | LR: 2.19e-04
2025-05-25 12:59:25,999 - INFO - Step 4394 | Batch 170/176 | Train Loss: 1.6448 | LR: 2.19e-04
2025-05-25 12:59:56,431 - INFO - Validation Loss: 8.4555
2025-05-25 12:59:56,520 - INFO - Epoch 25/50 Summary:
2025-05-25 12:59:56,521 - INFO - Train Loss: 1.0344 | Val Loss: 3.7418 | Best Val Loss: 0.9903
2025-05-25 12:59:56,521 - INFO - --------------------------------------------------
2025-05-25 12:59:56,524 - INFO - Epoch 26/50 | Learning Rate: 2.06e-04
2025-05-25 13:00:06,099 - INFO - Step 4410 | Batch 10/176 | Train Loss: 2.8045 | LR: 2.06e-04
2025-05-25 13:00:15,140 - INFO - Step 4420 | Batch 20/176 | Train Loss: 1.6284 | LR: 2.06e-04
2025-05-25 13:00:25,038 - INFO - Step 4430 | Batch 30/176 | Train Loss: 1.1211 | LR: 2.06e-04
2025-05-25 13:00:32,719 - INFO - Step 4440 | Batch 40/176 | Train Loss: 0.6435 | LR: 2.06e-04
2025-05-25 13:00:40,360 - INFO - Step 4450 | Batch 50/176 | Train Loss: 0.2980 | LR: 2.06e-04
2025-05-25 13:00:47,501 - INFO - Step 4460 | Batch 60/176 | Train Loss: 1.2764 | LR: 2.06e-04
2025-05-25 13:00:54,782 - INFO - Step 4470 | Batch 70/176 | Train Loss: 0.4163 | LR: 2.06e-04
2025-05-25 13:01:02,082 - INFO - Step 4480 | Batch 80/176 | Train Loss: 2.3557 | LR: 2.06e-04
2025-05-25 13:01:09,357 - INFO - Step 4490 | Batch 90/176 | Train Loss: 0.6630 | LR: 2.06e-04
2025-05-25 13:01:16,623 - INFO - Step 4500 | Batch 100/176 | Train Loss: 0.1974 | LR: 2.06e-04
2025-05-25 13:01:25,590 - INFO - Step 4510 | Batch 110/176 | Train Loss: 0.0228 | LR: 2.06e-04
2025-05-25 13:01:33,074 - INFO - Step 4520 | Batch 120/176 | Train Loss: 2.5088 | LR: 2.06e-04
2025-05-25 13:01:40,268 - INFO - Step 4530 | Batch 130/176 | Train Loss: 0.7293 | LR: 2.06e-04
2025-05-25 13:01:47,597 - INFO - Step 4540 | Batch 140/176 | Train Loss: 0.7797 | LR: 2.06e-04
2025-05-25 13:01:54,907 - INFO - Step 4550 | Batch 150/176 | Train Loss: 2.3115 | LR: 2.06e-04
2025-05-25 13:02:01,704 - INFO - Step 4560 | Batch 160/176 | Train Loss: 1.4337 | LR: 2.06e-04
2025-05-25 13:02:08,048 - INFO - Step 4570 | Batch 170/176 | Train Loss: 1.0341 | LR: 2.06e-04
2025-05-25 13:02:30,101 - INFO - Validation Loss: 16.4576
2025-05-25 13:02:30,547 - INFO - Epoch 26/50 Summary:
2025-05-25 13:02:30,547 - INFO - Train Loss: 0.9606 | Val Loss: 5.7757 | Best Val Loss: 0.9903
2025-05-25 13:02:30,548 - INFO - --------------------------------------------------
2025-05-25 13:02:30,550 - INFO - Epoch 27/50 | Learning Rate: 1.94e-04
2025-05-25 13:02:37,212 - INFO - Step 4586 | Batch 10/176 | Train Loss: 1.5236 | LR: 1.94e-04
2025-05-25 13:02:44,086 - INFO - Step 4596 | Batch 20/176 | Train Loss: 1.7472 | LR: 1.94e-04
2025-05-25 13:02:50,847 - INFO - Step 4606 | Batch 30/176 | Train Loss: 2.2450 | LR: 1.94e-04
2025-05-25 13:02:57,751 - INFO - Step 4616 | Batch 40/176 | Train Loss: 0.9098 | LR: 1.94e-04
2025-05-25 13:03:04,514 - INFO - Step 4626 | Batch 50/176 | Train Loss: 2.3112 | LR: 1.94e-04
2025-05-25 13:03:10,987 - INFO - Step 4636 | Batch 60/176 | Train Loss: 0.7378 | LR: 1.94e-04
2025-05-25 13:03:17,333 - INFO - Step 4646 | Batch 70/176 | Train Loss: 1.2101 | LR: 1.94e-04
2025-05-25 13:03:25,233 - INFO - Step 4656 | Batch 80/176 | Train Loss: 0.9019 | LR: 1.94e-04
2025-05-25 13:03:31,662 - INFO - Step 4666 | Batch 90/176 | Train Loss: 1.7503 | LR: 1.94e-04
2025-05-25 13:03:37,978 - INFO - Step 4676 | Batch 100/176 | Train Loss: 0.0681 | LR: 1.94e-04
2025-05-25 13:03:44,351 - INFO - Step 4686 | Batch 110/176 | Train Loss: 1.9635 | LR: 1.94e-04
